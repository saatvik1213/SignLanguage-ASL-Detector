{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4mUPzeAvE3wG",
    "outputId": "453907c0-7deb-4f79-d8fe-5ca26947fc7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (27455, 28, 28, 1)\n",
      "Test data shape: (7172, 28, 28, 1)\n",
      "Training labels shape: (27455, 25)\n",
      "Test labels shape: (7172, 25)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Load the training and test data\n",
    "train_file_path = 'C:/Users/saatv/downloads/archive/sign_mnist_train.csv'  # Replace with your actual training file path\n",
    "test_file_path = 'C:/Users/saatv/downloads/archive/sign_mnist_test.csv'    # Replace with your actual test file path\n",
    "\n",
    "train_data = pd.read_csv(train_file_path)\n",
    "test_data = pd.read_csv(test_file_path)\n",
    "\n",
    "# Separate features (pixels) and labels for training data\n",
    "X_train = train_data.drop('label', axis=1).values  # Extract pixel values for training\n",
    "y_train = train_data['label'].values  # Extract labels for training\n",
    "\n",
    "# Separate features (pixels) and labels for test data\n",
    "X_test = test_data.drop('label', axis=1).values  # Extract pixel values for testing\n",
    "y_test = test_data['label'].values  # Extract labels for testing\n",
    "\n",
    "# Reshape the image data from 1D to 2D (28x28) and normalize to range [0, 1]\n",
    "X_train_reshaped = X_train.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
    "X_test_reshaped = X_test.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
    "\n",
    "# Convert labels to categorical format (one-hot encoding) with 27 classes\n",
    "y_train_categorical = to_categorical(y_train, num_classes=25)\n",
    "y_test_categorical = to_categorical(y_test, num_classes=25)\n",
    "\n",
    "# Print shapes to confirm successful processing\n",
    "print(f\"Training data shape: {X_train_reshaped.shape}\")\n",
    "print(f\"Test data shape: {X_test_reshaped.shape}\")\n",
    "print(f\"Training labels shape: {y_train_categorical.shape}\")\n",
    "print(f\"Test labels shape: {y_test_categorical.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "AfbEFijlHQ9a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_3 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 3, 3, 128)         73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 1, 1, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 25)                6425      \n",
      "=================================================================\n",
      "Total params: 132,121\n",
      "Trainable params: 132,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "429/429 [==============================] - 22s 4ms/step - loss: 1.9131 - accuracy: 0.3962 - val_loss: 0.8174 - val_accuracy: 0.7171\n",
      "Epoch 2/20\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.5535 - accuracy: 0.8115 - val_loss: 0.4649 - val_accuracy: 0.8299\n",
      "Epoch 3/20\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.2361 - accuracy: 0.9221 - val_loss: 0.3746 - val_accuracy: 0.8829\n",
      "Epoch 4/20\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.1065 - accuracy: 0.9669 - val_loss: 0.3166 - val_accuracy: 0.8982\n",
      "Epoch 5/20\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.0603 - accuracy: 0.9822 - val_loss: 0.2792 - val_accuracy: 0.9090\n",
      "Epoch 6/20\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.0444 - accuracy: 0.9870 - val_loss: 0.2697 - val_accuracy: 0.9166\n",
      "Epoch 7/20\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.0257 - accuracy: 0.9934 - val_loss: 0.2668 - val_accuracy: 0.9096\n",
      "Epoch 8/20\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.0207 - accuracy: 0.9948 - val_loss: 0.3233 - val_accuracy: 0.9267\n",
      "Epoch 9/20\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.0265 - accuracy: 0.9912 - val_loss: 0.2628 - val_accuracy: 0.9317\n",
      "Epoch 10/20\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.0141 - accuracy: 0.9960 - val_loss: 0.4137 - val_accuracy: 0.9039\n",
      "Epoch 11/20\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.0235 - accuracy: 0.9927 - val_loss: 0.2647 - val_accuracy: 0.9354\n",
      "Epoch 12/20\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.0149 - accuracy: 0.9956 - val_loss: 0.3120 - val_accuracy: 0.9346\n",
      "Epoch 13/20\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.0098 - accuracy: 0.9970 - val_loss: 0.4130 - val_accuracy: 0.9233\n",
      "Epoch 14/20\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.0119 - accuracy: 0.9962 - val_loss: 0.3081 - val_accuracy: 0.9292\n",
      "Epoch 15/20\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.0079 - accuracy: 0.9977 - val_loss: 0.2183 - val_accuracy: 0.9384\n",
      "Epoch 16/20\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.0138 - accuracy: 0.9958 - val_loss: 0.2135 - val_accuracy: 0.9338\n",
      "Epoch 17/20\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.0104 - accuracy: 0.9969 - val_loss: 0.2597 - val_accuracy: 0.9335\n",
      "Epoch 18/20\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.0054 - accuracy: 0.9984 - val_loss: 0.2742 - val_accuracy: 0.9487\n",
      "Epoch 19/20\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.0052 - accuracy: 0.9981 - val_loss: 0.2644 - val_accuracy: 0.9488\n",
      "Epoch 20/20\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.0163 - accuracy: 0.9949 - val_loss: 0.2611 - val_accuracy: 0.9268\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 0.2611 - accuracy: 0.9268\n",
      "Test Accuracy: 0.93\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from collections import deque\n",
    "\n",
    "# Load your pre-trained CNN model\n",
    "cnn_model = load_model('SignLanguage.h5')  # Replace with your CNN model path\n",
    "\n",
    "# Define the LSTM model to capture temporal dependencies\n",
    "lstm_model = Sequential()\n",
    "lstm_model.add(LSTM(64, input_shape=(10, 128), return_sequences=True))\n",
    "lstm_model.add(LSTM(128, return_sequences=False))\n",
    "lstm_model.add(Dense(25, activation='softmax'))\n",
    "lstm_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Initialize the video capture object\n",
    "cap = cv2.VideoCapture(0)  # Use 0 for the default camera\n",
    "\n",
    "# To store the features extracted from each frame\n",
    "frame_queue = deque(maxlen=10)  # Keep last 10 frames for temporal analysis\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Preprocess the frame\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    resized_frame = cv2.resize(gray_frame, (28, 28))\n",
    "    normalized_frame = resized_frame.astype('float32') / 255.0\n",
    "    reshaped_frame = np.reshape(normalized_frame, (1, 28, 28, 1))\n",
    "\n",
    "    # Extract features from the frame using the CNN model\n",
    "    frame_features = cnn_model.predict(reshaped_frame)\n",
    "\n",
    "    # Store the frame features in the deque\n",
    "    frame_queue.append(frame_features.flatten())\n",
    "\n",
    "    # When we have enough frames, use LSTM to predict the sign language\n",
    "    if len(frame_queue) == 10:\n",
    "        # Prepare the input for the LSTM model\n",
    "        lstm_input = np.array(frame_queue)\n",
    "        lstm_input = np.reshape(lstm_input, (1, 10, 128))  # Reshape to (batch_size, timesteps, features)\n",
    "\n",
    "        # Predict the sign language\n",
    "        prediction = lstm_model.predict(lstm_input)\n",
    "        predicted_class = np.argmax(prediction)\n",
    "\n",
    "        # Display the predicted class\n",
    "        print(f\"Predicted sign: {predicted_class}\")\n",
    "\n",
    "    # Display the original video feed\n",
    "    cv2.imshow('Sign Language Video Translation', frame)\n",
    "\n",
    "    # Exit on pressing 'q'\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c c c c c c c i i i b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i m m m m m m m m m m m m m m m q q q q q m m m m m m m m m q m m m m m m m m m m m q q q q q q q q q q q q q q q q q q q q q q q q q q m m m m m m m m q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q m m m m m m m m m m m q m m m m m m m m m m m o m m m m m m m m m o o o o o o m m m m m m o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o m m m m m m m i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i b b b b b b b b i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i m m m m m m m m m m m m m m b i i i i i i i i i i i i b i i i b b b b b m m m m f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f b "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from collections import deque\n",
    "\n",
    "# Load pre-trained CNN model\n",
    "cnn_model = load_model('SignLanguage.h5')\n",
    "\n",
    "# Extract features from the second-to-last layer\n",
    "feature_extractor = Model(inputs=cnn_model.input, outputs=cnn_model.layers[-2].output)\n",
    "\n",
    "# Define the LSTM model for temporal analysis\n",
    "lstm_model = Sequential()\n",
    "lstm_model.add(LSTM(64, input_shape=(10, 256), return_sequences=True))  # Adjusted input shape\n",
    "lstm_model.add(LSTM(128, return_sequences=False))\n",
    "lstm_model.add(Dense(25, activation='softmax'))\n",
    "lstm_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Initialize the video capture object\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# To store the features extracted from each frame\n",
    "frame_queue = deque(maxlen=10)  # Keep last 10 frames for temporal analysis\n",
    "\n",
    "# Translation unit: 0 -> 'a', 1 -> 'b', ..., 25 -> 'z'\n",
    "letters = {i: chr(97 + i) for i in range(26)}  # chr(97) is 'a'\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Preprocess the frame\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    resized_frame = cv2.resize(gray_frame, (28, 28))\n",
    "    normalized_frame = resized_frame.astype('float32') / 255.0\n",
    "    reshaped_frame = np.reshape(normalized_frame, (1, 28, 28, 1))\n",
    "\n",
    "    # Extract features from the frame using the CNN model\n",
    "    frame_features = feature_extractor.predict(reshaped_frame)\n",
    "\n",
    "    # Store the frame features in the deque\n",
    "    frame_queue.append(frame_features.flatten())\n",
    "\n",
    "    # When we have enough frames, use LSTM to predict the sign language\n",
    "    if len(frame_queue) == 10:\n",
    "        lstm_input = np.array(frame_queue)\n",
    "        lstm_input = np.reshape(lstm_input, (1, 10, 256))  # Correct shape for LSTM\n",
    "\n",
    "        # Predict the sign language\n",
    "        prediction = lstm_model.predict(lstm_input)\n",
    "        predicted_class = np.argmax(prediction)\n",
    "\n",
    "        # Translate the predicted class to the corresponding letter\n",
    "        predicted_letter = letters.get(predicted_class, '')  # Default to empty if not found\n",
    "        print(predicted_letter, end=' ')  # Print the letter with a space separator\n",
    "\n",
    "    # Display the original video feed\n",
    "    cv2.imshow('Sign Language Video Translation', frame)\n",
    "\n",
    "    # Exit on pressing 'q'\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
